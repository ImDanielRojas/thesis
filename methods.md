## Methods
### Generative Adversarial Networks
Generative Adversarial Networks (GANs) are a type of generative model composed of two different networks: the generator and the discriminator. The generator's objective is to generate fake data indistinguishible from real data. On the other hand, the discriminator deals with classifying whether a random sample has been artificially generated or is a real sample data. Both networks will compete during training in which the gain or loss of one of the networks is offset by the gain or loss of the opposite, till the discriminator cannot correctly identify between the samples.

The generator, U-Net, consists of an encoder-decoder architecture in which a convolution is applied after the last layer in the decoder to map the number of output channels, 170 channels in our case.
The discriminator will decide whether the unknown image was generated from the generator or not. The architecture is also called a PatchGAN and will decide if patches from the sub-image patches are real or not.
We will focus out study around the Pix2Pix model, a state-of-the-art framework for image translation based on GANs extracting correspondence features between pairs of images. This model is a general-purpose solution for image translation tasks and is sometimes limited due to this generalization, which makes it non-specialized in capturing relationships between original and constructed images of specific constraints, characteristics and abstractions.
Transforming Multispectral data to Hyperspectral data is a clear example of a task and data that differs from the standard example solutions in which Pix2Pix is often showed performing.
Pix2Pix is based on Conditional Generative Adversarial Network (CGAN), whose generator network learns a mapping from the input image $x$ to the target image y, i.e., x->y and the discriminator network will try to classify y|x, real or fake.

### SAM-GAN: Proposed enhancement
Most of the loss functions used in architectures are based on two-dimensional feature extraction, computing the error between data extracting them channel by channel. When dealing with hyperspectral data, the most important features and relationships to aim to be extracted are not only on the spatial dimension but on the spectral dimension too. In this study, we propose a loss function that gathers both high-level features from the spatial dimension and the long spectral dimension in order to achieve a solid learning of the structure of the HS data.
The loss function is based on pix2pix's loss function but adding the Spectral Angle Mapper (SAM) metric to minimize the errors of spectral features. SAM qualifies the similarity of the original and the transformed vector reflectance across the spectra through measuring the average angle between them.
